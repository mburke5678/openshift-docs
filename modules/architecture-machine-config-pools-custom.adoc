// Module included in the following assemblies:
//
// * architecture/control-plane.adoc

:ign-config-version: 3.2.0

:_content-type: REFERENCE
[id="architecture-machine-config-pools-custom_{context}"]
= Configuring a node by using custom machine config pools

You can create custom machine config pools that give you the ability to deploy changes that are targeted at specific nodes. One example of this is an infrastructure node that hosts only infrastructure components, such as the router and the image registry.   

For information on infra nodes, see the Additional resources at the end of this section.

Custom machine config pools are based on the worker machine config pool. As such, custom pools inherit their initial configuration from the worker pool and any change to the worker pool is applied to the custom pool. Changes to the custom pool are not inherited by the worker pool.

[NOTE]
====
You cannot use a master machine config pool to create a custom pool.
====

Note the following about nodes and machine config pools:

* A node can only be included in one machine config pool. 
* A node must be included in a machine config pool for the node to function properly.
* If a node has multiple labels that correspond to different pools, such as `worker,infra`, the node is managed by the custom pool, not the worker pool. Custom pools take priority on selecting nodes to manage based on node labels.
* If a node has the `worker,infra` label, but you do not have an infra machine config pool, the Machine Config Operator (MCO) considers the node a worker node.
* If a node has only a custom label that does not have a corresponding custom pool, the node is not recognized by the MCO and is unmanaged by the cluster.

Because the custom pool inherits its configuration from the worker machine config pool by default, a purely custom node stills get updated when a worker machine config is updated. However, workloads scheduled for worker nodes are no longer scheduled on the custom node.

[NOTE]
====
If a custom node reboots, the node reboots into the `worker` machine config pool. You need to reapply the custom label to that node to place the node back in the custom pool. 
====

About creating the custom machine config pool:: Create a `MachineConfigPool` object that contains a custom role as the `machineConfigSelector`. 
+
The following example creates a custom machine config pool named `infra` that is also associated with the `worker` machine config pool: 
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} <1>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: ""
----
<1> You must include `worker` as a value, even if you remove the `worker` label from your custom node.
+
The `oc get mcp` output shows the new machine config pool:
+
.Example output
[source,terminal]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
custom   rendered-infra-b5250cb9e8c4a0b9d3e16da185ba4bad    True      False      True       0              0                   0                     0                      15s
master   rendered-master-b1d4b6196a18869d63518e57b8b335d1   True      False      False      3              3                   3                     0                      156m
worker   rendered-worker-c48ef626ff01f0a445979a4245dc7bdd   True      False      False      3              3                   3                     0                      156m
----
+
To add a node to a custom machine config pool, first add a custom role label to the node, for example: `infra`:
+
[source,terminal]
----
$ oc label node ip-10-0-130-218.us-west-1.compute.internal node-role.kubernetes.io/infra=
----
+
The `oc get nodes` output shows that node with the `worker` and custom roles.
+
.Example output
[source,terminal]
----
NAME                                         STATUS    ROLES          AGE   VERSION
ip-10-0-130-218.us-west-1.compute.internal   Ready     infra,worker   37m   v1.14.0+e020ea5b3
ip-10-0-131-9.us-west-1.compute.internal     Ready     master         43m   v1.14.0+e020ea5b3
ip-10-0-134-237.us-west-1.compute.internal   Ready     master         43m   v1.14.0+e020ea5b3
ip-10-0-138-167.us-west-1.compute.internal   Ready     worker         37m   v1.14.0+e020ea5b3
ip-10-0-151-146.us-west-1.compute.internal   Ready     master         43m   v1.14.0+e020ea5b3
ip-10-0-152-59.us-west-1.compute.internal    Ready     worker         37m   v1.14.0+e020ea5b3
----
+
If you want to use the node with only the custom role, remove the worker role label, for example:
+
[source,terminal]
----
$ oc label node ip-10-0-130-218.us-west-1.compute.internal node-role.kubernetes.io/worker-
----
+
The `oc get nodes` output shows that node with only the custom role.
+
.Example output
[source,terminal]
----
NAME                                         STATUS    ROLES          AGE   VERSION
ip-10-0-130-218.us-west-1.compute.internal   Ready     infra          38m   v1.14.0+e020ea5b3
ip-10-0-131-9.us-west-1.compute.internal     Ready     master         44m   v1.14.0+e020ea5b3
ip-10-0-134-237.us-west-1.compute.internal   Ready     master         44m   v1.14.0+e020ea5b3
ip-10-0-138-167.us-west-1.compute.internal   Ready     worker         34m   v1.14.0+e020ea5b3
ip-10-0-151-146.us-west-1.compute.internal   Ready     master         44m   v1.14.0+e020ea5b3
ip-10-0-152-59.us-west-1.compute.internal    Ready     worker         34m   v1.14.0+e020ea5b3
----

About making changes to a custom pool:: To deploy changes to a custom pool, create a `MachineConfig` object that uses the custom pool as the label, for example:
+
[source,yaml,subs="attributes+"]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: infra
  name: 51-infra
spec:
  config:
    ignition:
      version: {ign-config-version}
    storage:
      files:
      - contents:
          source: data:,infra
        filesystem: root
        mode: 0644
        path: /etc/infratest
----
+
After a node returns to the `Ready` state, you can see that the machine config pool has been applied:
+
[source,terminal]
----
$ oc get mcp
----
+
.Example output
[source,terminal]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-custom-11f6e3ee8704373add6d3c3d8399b3fa   True      False      False      1              1                   1                     0                      11m
master   rendered-master-94cafd2f05c7cd9a3fce4877e699fa45   True      False      False      3              3                   3                     0                      119m
worker   rendered-worker-93ac02b5fc2e2526b8b15758df3d8472   True      False      False      2              2                   2                     0                      119m
----
+
Changes made to the custom machine config are not applied to the `worker` machine config.
+
However, if you make a change to the `worker` machine config pool, the change is applied to the `worker` and custom machine config pools and their associated nodes. 
+
[source,terminal]
----
$ oc get mcp
----
+
.Example output
[source,terminal]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-custom-e7c94136ebb9f655bb20d7da14eb6c1f   False     True       False      1              0                   0                     0                      74m
master   rendered-master-6e3f6a6544848b5e1d8fccf57283263d   True      False      False      3              3                   3                     0                      113m
worker   rendered-worker-0a1b442474e43fee1d60a4b9aeb79ca0   False     True       False      2              0                   0                     0                      113m
----
+
[source,terminal]
----
$ oc get mc
----
+
[source,terminal]
----
rendered-infra-2ac5bfa5ab0950a344feb9d994f7eafd    b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             5s <1>
rendered-infra-70be77bfe06c59869c66b64c12be9524    b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             47m
rendered-infra-b5250cb9e8c4a0b9d3e16da185ba4bad    b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             37m
rendered-infra-cb73231c08063f313b42225013ee909b    b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             11m
rendered-master-142c359bb4f3b4f6f31c5ec154219ea4   b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             77m
rendered-worker-70be77bfe06c59869c66b64c12be9524   b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             77m
rendered-worker-d177d1547e6d6d5820876783942b6fc9   b29dceb12638f3d7f30f339729344de69824a09b   3.2.0             5s <2>
----
<1> New rendered custom machine config in response to the change.
<2> New rendered worker machine config in response to the change.

About removing a node from a custom machine config:: To remove a node from a custom machine config pool, remove the associated role label from the custom node. 
+
Because each node must have a role at all times to properly function. If you have a purely custom node, you should first apply a worker role label to the node before you remove the custom role label.
+
[source,terminal]
----
$ oc label node ip-10-0-130-218.us-west-1.compute.internal node-role.kubernetes.io/worker=
----
+
[source,terminal]
----
$ oc label node ip-10-0-130-218.us-west-1.compute.internal node-role.kubernetes.io/infra-
----
+
You can see the node was removed from the custom machine config pool and returned to the `worker` config pool.
+
.Example output
[source,terminal]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
custom   rendered-custom-11f6e3ee8704373add6d3c3d8399b3fa   True      False      False      0              0                   0                     0                      18m
master   rendered-master-94cafd2f05c7cd9a3fce4877e699fa45   True      False      False      3              3                   3                     0                      125m
worker   rendered-worker-93ac02b5fc2e2526b8b15758df3d8472   False     True       False      3              3                   3                     0                      125m
----
